/*
* EDDL Library - European Distributed Deep Learning Library.
* Version: 0.9
* copyright (c) 2021, Universidad Polit√©cnica de Valencia (UPV), PRHLT Research Centre
* Date: July 2021
* Author: PRHLT Research Centre, UPV, (rparedes@prhlt.upv.es), (jon@prhlt.upv.es)
* All rights reserved
*
*/

#include <cstdio>
#include <cstdlib>
#include <ctime>
#include <iostream>

#include "eddl/apis/eddl.h"
#include "eddl/serialization/onnx/eddl_onnx.h"


using namespace eddl;

//////////////////////////////////
// Testing issue generated by George Mavrakis from WINGS
// Using train_batch() for training
//////////////////////////////////

bool reduced = false;

int main(int argc, char **argv)
{
    // Default settings
    int epochs = 1;
    int batch_size = 50;
    bool use_cpu = false;
    bool output_softmax = true;
    bool debug = false;
    bool create_a_new_net = false;
    std::string optimizer_type = "adam";

    for (int i = 1; i < argc; ++i) {
        if (strcmp(argv[i], "--cpu") == 0) {
            use_cpu = true;
        } else if (strcmp(argv[i], "--epochs") == 0) {
            epochs = atoi(argv[++i]);
        } else if (strcmp(argv[i], "--batch-size") == 0) {
            batch_size = atoi(argv[++i]);
        } else if (strcmp(argv[i], "--softmax-at-output") == 0) {
            output_softmax = true;
        } else if (strcmp(argv[i], "--sigmoid-at-output") == 0) {
            output_softmax = false;
        } else if (strcmp(argv[i], "--debug") == 0) {
            debug = true;
        } else if (strcmp(argv[i], "--create") == 0) {
            create_a_new_net = true;
        } else if (strcmp(argv[i], "--reduced") == 0) {
            reduced = true;
        } else if (strcmp(argv[i], "--optimizer") == 0) {
            optimizer_type = std::string(argv[++i]);
        }
    }


    // Settings
    int num_classes = 2;

    // Define network
    layer in, out;
    model net;
    if (create_a_new_net) {
        in = Input({26});
        layer l = in; 

        //l = ReLu(BatchNormalization(RNN(l, 256)));
        l = ReLu(BatchNormalization(GRU(l, 256), true, 0.99f));
        //l = ReLu(BatchNormalization(LSTM(l, 256)));
        l = ReLu(BatchNormalization(Dense(l, 256), true, 0.99f));
        /*
        l = ReLu(GRU(l, 256));
        l = ReLu(Dense(l, 256));
        */

        if (output_softmax)
            out = Softmax(Dense(l, num_classes), 1);
        else
            out = Sigmoid(Dense(l, 1));
        net = Model({in}, {out});
        net->verbosity_level = 0;
    } else {
        string filename = string("models/rnn-with-output-") + (output_softmax ? "softmax" : "sigmoid") + "-start.onnx";
        net = import_net_from_onnx_file(filename);
    }

    // dot from graphviz should be installed:
    // plot(net, "model.pdf");

    compserv cs = nullptr;
    if (use_cpu) {
        cs = CS_CPU();
    } else {
        cs = CS_GPU({1}, "full_mem"); // one GPU
    }

    optimizer opt = nullptr;
    if (optimizer_type == "adam")
        opt = adam(1.0e-3, 0.9f, 0.999f, 1.0e-3f); // lr = 1.0e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1.0e-3
    else if (optimizer_type == "sgd")
        opt = sgd(1.0e-3);
    else if (optimizer_type == "rmsprop")
        opt = rmsprop(1.0e-3, 0.9f, 1.0e-3f); // lr = 1.0e-3, rho = 0.9, epsilon = 1.0e-3
    else
        msg("optimizer not recognized");

    // Build model
    build(net, opt,
          {output_softmax ? "softmax_cross_entropy" : "binary_cross_entropy"}, // Losses
          {output_softmax ? "categorical_accuracy" : "binary_accuracy"}, // Metrics
          cs,
          create_a_new_net);

    if (create_a_new_net) {
        string filename = string("models/rnn-with-output-") + (output_softmax ? "softmax" : "sigmoid") + "-start.onnx";
        save_net_to_onnx_file(net, filename);
    }

    // View model
    summary(net);


    // Load data
    Tensor * X_train = Tensor::load("data/X_train.bin");
    Tensor * y_train = Tensor::load("data/y_train.bin");
    Tensor * X_test  = Tensor::load("data/X_test.bin");
    Tensor * y_test  = Tensor::load("data/y_test.bin");
    vector<int> shape_Y_train = {y_train->getShape()[0], 1, output_softmax ? 2 : 1};
    vector<int> shape_Y_test  = {y_test->getShape()[0],  1, output_softmax ? 2 : 1};
    Tensor * Y_train = Tensor::zeros(shape_Y_train);
    Tensor * Y_test  = Tensor::zeros(shape_Y_test);
    Tensor * z_train = Tensor::zeros(y_train->getShape());
    Tensor * z_test  = Tensor::zeros(y_test->getShape());

    //X_train->fill_rand_normal_(0.0f, 1.0f);
    //X_test->fill_rand_normal_(0.0f, 1.0f);

    if (output_softmax) {
        int stride = Y_train->stride[0];
        for (int i = 0; i < shape_Y_train[0]; i++) Y_train->ptr[i * stride + (y_train->ptr[i] == 1)] = 1;
        stride = Y_test->stride[0];
        for (int i = 0; i < shape_Y_test[0];  i++)  Y_test->ptr[i * stride + ( y_test->ptr[i] == 1)] = 1;
    } else {
        Tensor::copy(y_train, Y_train);
        Tensor::copy(y_test, Y_test);
    }


    if (debug) {
        X_train->info();
        printf("min(X_train) = %.6f   max(X_train) = %.6f\n", X_train->min(), X_train->max());
        y_train->info();
        Y_train->info();
    }


    // Preprocessing
    float mean = X_train->mean();
    float std = X_train->std();

    /*
    X_train->clamp_(mean - 3 * std, mean + 3 * std);
    X_test->clamp_(mean - 3 * std, mean + 3 * std);
    */
    
    X_train->sub_(mean); X_train->div_(std);
     X_test->sub_(mean);  X_test->div_(std);

    Tensor * xbatch = new Tensor({batch_size, X_train->shape[1], X_train->shape[2]});
    Tensor * ybatch = new Tensor({batch_size, Y_train->shape[1], Y_train->shape[2]});

    // Train model
    for (int i = 0; i < (reduced ? 1 : epochs); i++) {
        /*
        fit(net, {X_train}, {Y_train}, batch_size, 1);
        evaluate(net, {X_test}, {Y_test}, batch_size);
        */
        reset_loss(net);
        for (int j = 0; j < (reduced ? 2 : std::ceil(1.0 * X_train->shape[0] / batch_size)); j++) {

            //next_batch({X_train, Y_train}, {xbatch, ybatch});
            ////////////////////////////////////////////////////////////////////////////
            ///// TO DO NOT SHUFLE TRAINING SAMPLES ////////////////////////////////////
            ///// AND TO DO NOT CHANGE THE NUMBER OF SAMPLES IN THE LAST BATCH /////////
            ////////////////////////////////////////////////////////////////////////////
            delete xbatch;
            delete ybatch;
            char batch_range[128];
            int b_to = min(X_train->shape[0], (j + 1) * batch_size);
            int b_from = b_to - batch_size;
            sprintf(batch_range, "%d:%d", b_from, b_to);
            xbatch = X_train->select({batch_range, ":", ":"});
            ybatch = Y_train->select({batch_range, ":", ":"});
            ////////////////////////////////////////////////////////////////////////////

            train_batch(net, {xbatch}, {ybatch});

            printf("epoch %d  %s  ", i, batch_range);
            print_loss(net, j + 1);
            printf("\r");
        }
        printf("\n");
    }

    void print_results(model, Tensor *, Tensor *, Tensor *, Tensor *, int, bool, int, const char *);

    print_results(net, X_train, y_train, Y_train, z_train, batch_size, debug, epochs, output_softmax ? "softmax" : "sigmoid");
    print_results(net, X_test,  y_test,  Y_test,  z_test,  batch_size, debug, epochs, output_softmax ? "softmax" : "sigmoid");

    delete X_train;
    delete y_train;
    delete Y_train;
    delete z_train;
    delete X_test;
    delete y_test;
    delete Y_test;
    delete z_test;
    delete xbatch;
    delete ybatch;
    delete net;


    //system("tail -n 30 report-cpp.log");
    printf("\n\nRUN the following command to see results:\n\n   tail -n 30 report-cpp.log\n\n");
    
    return EXIT_SUCCESS;
}


void print_results(model net, Tensor * X, Tensor * y, Tensor * Y, Tensor * z, int batch_size, bool debug, int epochs, const char * output_activation)
{
    net->setmode(TSMODE);
    for (int i = 0; i < (reduced ? 2 * batch_size : X->shape[0]); i += batch_size) {
        char batch_range[128];
        sprintf(batch_range, "%d:%d", i, min(X->shape[0], i + batch_size));
        Tensor * sample = X->select({batch_range, ":", ":"});
        Tensor * output, * temp;
        vtensor voutput = net->predict({sample});
        output = voutput[0];

        if (output->shape.size() > 2) {
            msg("Unexpected shape for output!");
        } else if (output->shape[1] == 2) {
            if (strcmp(output_activation, "softmax") != 0) msg("something is wrong!");
            if (debug) output->info();
            temp = output;
            output = temp->argmax({1}, false);
            delete temp;
            if (debug) output->info();
        } else 
            if (strcmp(output_activation, "sigmoid") != 0) msg("something is wrong!");

        for (int b = 0; b < batch_size; b++)
            if (i + b < z->shape[0])
                z->ptr[i + b] = output->ptr[b];

        delete output;
        delete sample;
    }
    
    float tp = 0, fp = 0, tn = 0, fn = 0, p = 0, n = 0;
    for (int i = 0; i < z->shape[0]; i++) {
        if (y->ptr[i] == 0) { ++n; if (z->ptr[i] < 0.5) ++tn; else ++fp; }
        else                { ++p; if (z->ptr[i] < 0.5) ++fn; else ++tp; }
    }

    FILE * log_file = fopen("report-cpp.log", "at");
    time_t timestamp;
    time(&timestamp);

    fprintf(log_file, "\n\n");
    fprintf(log_file, "RUN at %s", asctime(localtime(&timestamp)));
    fprintf(log_file, "    using %s as the activation for the output layer\n", output_activation);
    fprintf(log_file, "    trained during %d epochs\n", epochs);
    fprintf(log_file, "---------------------------------------\n");
    fprintf(log_file, "    %6d    %6d         accuracy = %.8f\n", 0, 1, (tn + tp) / (n + p));
    fprintf(log_file, "---------------------------------------\n");
    fprintf(log_file, "0   %6.0f    %6.0f   TN, FP\n", tn, fp);
    fprintf(log_file, "1   %6.0f    %6.0f   FN, TP\n", fn, tp);
    fprintf(log_file, "---------------------------------------\n");
    fprintf(log_file, "    %6.0f    %6.0f   N,  P  (total)\n", n, p);
    fprintf(log_file, "---------------------------------------\n");
    fprintf(log_file, "\n\n");

    fclose(log_file);
}
